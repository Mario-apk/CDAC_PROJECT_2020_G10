# -*- coding: utf-8 -*-
"""SVM model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ATonX1gHmpED0AOBLd2vdX8zrbNrhn_G

# Import Libraries
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve

emp=pd.read_csv("EmployeeData.csv")

emp = emp.rename(columns={'satisfaction_level': 'satisfaction', 
                        'last_evaluation': 'evaluation',
                        'number_project': 'projectCount',
                        'average_montly_hours': 'averageMonthlyHours',
                        'time_spend_company': 'yearsAtCompany',
                        'Work_accident': 'workAccident',
                        'promotion_last_5years': 'promotion',
                        'sales' : 'department',
                        'left' : 'turnover'
                        })

emp = emp[['turnover','satisfaction','evaluation','projectCount','averageMonthlyHours','yearsAtCompany','workAccident','promotion','department','salary'
         ]]

emp.department.unique()

emp.salary.unique()

emp.shape

turnover_rate = emp.turnover.value_counts() / 14999
turnover_rate

salary = {'low':0, 'medium':1,'high':2}
emp['salary'] = emp['salary'].map(lambda x : salary[x])

emp_obj = pd.get_dummies(emp,\
        columns=['department'])
emp_obj

"""# Train Test Split"""

from sklearn.preprocessing import MinMaxScaler
target_name = 'turnover'
X = emp_obj.drop('turnover', axis=1)
minmaxscaler = MinMaxScaler()
X=minmaxscaler.fit_transform(X)
y=emp_obj['turnover']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=30)

"""# SVM

SVM constructs a hyperplane in multidimensional space to separate different classes.

SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. 

The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.


Support Vectors

Support vectors are the data points, which are closest to the hyperplane. 

These points will define the separating line better by calculating margins. 

These points are more relevant to the construction of the classifier.

Hyperplane

A hyperplane is a decision plane which separates between a set of objects having different class memberships.

Margin

A margin is a gap between the two lines on the closest class points.

This is calculated as the perpendicular distance from the line to support vectors or closest points. 

If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.

Linear Kernel

A linear kernel can be used as normal dot product any two given observations. 

The product between two vectors is the sum of the multiplication of each pair of input values.
"""

from sklearn.svm import SVC
svc_linear_model = SVC(kernel='linear')
svc_linear_model.fit(X_train,y_train)

y_pred = svc_linear_model.predict(X_test)

"""A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. 
The matrix compares the actual target values with those predicted by the machine learning model.

This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.

Precision TP / (TP+ FP)

where FP is the number of true positives and FP the number of false positives.

The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.

Recall = TP / (TP + FN)

where TP is the number of true positives and FN the number of false negatives.

The recall is intuitively the ability of the classifier to find all the positive samples.

F Score = 2*Recall*Precision/Recall+Precision
    
It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.

The support is the number of occurrences of each class in y_true.

TP = Predict Employees will not leave the company and he didnt left 

TN = Predict Employees will  leave  the company and he left

FP = Predict Employees will leave the company but he didnt left

FN = Predict Employees will not leave the company but he left
"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

"""Polynomial Kernel

A polynomial kernel is a more generalized form of the linear kernel. 

The polynomial kernel can distinguish curved or nonlinear input space.
"""

svc_poly_model = SVC(kernel='poly',degree=4)
svc_poly_model.fit(X_train,y_train)
y_pred = svc_poly_model.predict(X_test)

print(classification_report(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))



